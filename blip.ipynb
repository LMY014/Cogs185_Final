{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import os, json\n",
        "from tqdm import tqdm  # Optional progress bar\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Step 1: Load CIFAR-10 dataset (raw images, not transformed yet)\n",
        "cifar10 = datasets.CIFAR10(root=\"data/\", train=True, download=True)\n",
        "\n",
        "# Step 2: Load BLIP captioning model\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\", use_fast=False)\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "# Step 3: Generate captions\n",
        "output = {}\n",
        "\n",
        "for idx in tqdm(range(100)):  # Try small batch first\n",
        "    image, label = cifar10[idx]\n",
        "\n",
        "    # üîÅ Resize and convert image to match BLIP expectations\n",
        "    image = image.resize((224, 224)).convert(\"RGB\")  # PIL image\n",
        "\n",
        "    # ‚ö†Ô∏è Important: do NOT call .to(device) on a dict directly\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move each tensor to GPU\n",
        "\n",
        "    out = model.generate(**inputs)\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    output[f\"cifar10_{idx}.png\"] = {\n",
        "        \"label\": cifar10.classes[label],\n",
        "        \"caption\": caption\n",
        "    }\n",
        "\n",
        "# Step 4: Save captions\n",
        "with open(\"cifar10_captions.json\", \"w\") as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Done! Captions saved to cifar10_captions.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6yq4wCRX5yV",
        "outputId": "2c6d34a8-dd7d-468e-f24e-fdd565873833"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:05<00:00, 33.8MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:38<00:00,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Done! Captions saved to cifar10_captions.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"cifar10_captions.json\") as f:\n",
        "    captions = json.load(f)\n",
        "\n",
        "for k, v in list(captions.items())[:5]:\n",
        "    print(f\"{k} ‚Üí label: {v['label']} | caption: {v['caption']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqQMTA2QYeVk",
        "outputId": "547e4f51-a432-44e5-eb38-c4b6c79e5dbb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar10_0.png ‚Üí label: frog | caption: a plate of food with a lot of food on it\n",
            "cifar10_1.png ‚Üí label: truck | caption: a large truck driving down a road\n",
            "cifar10_2.png ‚Üí label: truck | caption: a tractor is parked on the road\n",
            "cifar10_3.png ‚Üí label: deer | caption: a man in a suit and tie standing in the dark\n",
            "cifar10_4.png ‚Üí label: automobile | caption: a car is parked on the side of the road\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# blip_captioning.py\n",
        "\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import os, json\n",
        "\n",
        "# ‚úÖ Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ‚úÖ Load BLIP model\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\", use_fast=False)\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", use_safetensors=True).to(device)\n",
        "\n",
        "# ‚úÖ Input image folder (placeholder ‚Äì plug in real images later)\n",
        "image_dir = \"images_input\"  # e.g., contains 00001.jpg, 00002.jpg...\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "# ‚úÖ Output JSON\n",
        "caption_output_path = \"cifar10_captions.json\"\n",
        "output = {}\n",
        "\n",
        "# ‚úÖ Loop through images in folder\n",
        "image_filenames = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.png'))])\n",
        "\n",
        "for fname in image_filenames:\n",
        "    try:\n",
        "        path = os.path.join(image_dir, fname)\n",
        "        image = Image.open(path).convert(\"RGB\").resize((224, 224))\n",
        "\n",
        "        # Prepare and move input to device\n",
        "        inputs = processor(images=image, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Generate caption\n",
        "        out = model.generate(**inputs)\n",
        "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "        output[fname] = caption\n",
        "        print(f\"{fname} ‚Üí {caption}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Skipped {fname}: {e}\")\n",
        "\n",
        "# ‚úÖ Save all captions\n",
        "with open(caption_output_path, \"w\") as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Captions saved to {caption_output_path}\")\n"
      ],
      "metadata": {
        "id": "1AnM0bh2ae-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dycova19Y2hf"
      }
    }
  ]
}